import warnings
warnings.filterwarnings("ignore")

import gradio as gr
from typing import List

from .llm_api_client import MistralAPIClient

#Instantiate a LLMAPI client
client = MistralAPIClient()

def predict(message: str, history: List[List[str]], about_me: str):
    """
    Predicts a response to a given query using the LLM Client.

    Args:
        message (str): The query provdided by the user for response generation.
        history (List[List[str]]): A list of previous conversations.
        
    Returns:
        str: The response generated by the model.
    """
    
    #Stream the answer to the query from the LLM client
    for text in client.stream_answer(message, history):
        yield text


############## === Gradio chat Interface ===

demo = gr.ChatInterface(
    predict,
    textbox=gr.Textbox(
        placeholder="Ask me any question",
        label="question",
        container=False,
        scale=5,
    ),
    title="Diya - The Sassy AI Assistant",
    description="Ask me any question about myself of about my boss Aakash, and I will do my best to answer them.",
    theme="soft",
    examples=[
        [
            "Hi There! What is your name?"
        ],
        [
            "Where does Aakash Currently Works?"
        ],
        [
            "What is the favourite food of Aakash?"
        ],
        [
            "What does Aakash loves to do in his free time?"
        ]
    ],
    cache_examples=False,
    retry_btn=None,
    undo_btn=None,
    clear_btn="Clear",
)


if __name__ == "__main__":
    demo.queue(api_open=False).launch(server_name="0.0.0.0", server_port=7860, share=False, show_api=False) #Launch the web app UI.